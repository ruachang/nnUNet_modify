{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: How to reproduce nnUNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the file sturcture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Build a new directory to store both the network and data\n",
    "2. Git clone the rep from (https://github.com/MIC-DKFZ/nnUNet.git)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import shutil\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "# * default path of the new testing network\n",
    "my_nnunet_dir = os.path.join(root_dir,'my_nnunet')\n",
    "input_dir = os.path.join(root_dir, 'input')\n",
    "ground_truth_dir = os.path.join(root_dir, 'ground_truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_if_dont_exist(folder_path,overwrite=False):\n",
    "    \"\"\"\n",
    "    creates a folder if it does not exists\n",
    "    input:\n",
    "    folder_path : relative path of the folder which needs to be created\n",
    "    over_write :(default: False) if True overwrite the existing folder\n",
    "    \"\"\"\n",
    "    if os.path.exists(folder_path):\n",
    "\n",
    "        if not overwrite:\n",
    "            print(f'{folder_path} exists.')\n",
    "        else:\n",
    "            print(f\"{folder_path} overwritten\")\n",
    "            shutil.rmtree(folder_path)\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "    else:\n",
    "      os.makedirs(folder_path)\n",
    "      print(f\"{folder_path} created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_nnunet created!\n",
      "input created!\n",
      "ground_truth created!\n",
      "Current working directory: f:\\dataset\\mindBoggle\\nnUNet\\my_nnunet\n"
     ]
    }
   ],
   "source": [
    "# * go to that dir and make/overwrite the path\n",
    "os.chdir(root_dir)\n",
    "make_if_dont_exist('my_nnunet', overwrite=False)\n",
    "make_if_dont_exist('input', overwrite=False)\n",
    "make_if_dont_exist('ground_truth', overwrite=False)\n",
    "\n",
    "os.chdir('my_nnunet')\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Follow the readme of the rep: using `pip install e .` to download all dependencies, after install, to change the content of the file that is import, we should change the file in the path `/opt/conda/lib/python3.10/site-packages/nnunetv2/`\n",
    "4. (Select) download hidden layers config with `pip install --upgrade git+https://github.com/nanohanno/hiddenlayer.git@bugfix/get_trace_graph#egg=hiddenlayer` or \n",
    "```\n",
    "git clone --branch bugfix/get_trace_graph https://github.com/nanohanno/hiddenlayer.git\n",
    "cd hiddenlayer\n",
    "pip install --upgrade .\n",
    "```\n",
    "5. setup the environment variables for program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make legal dataset for nnUNet\n",
    "\n",
    "***Requirements***\n",
    "\n",
    "* Training case:\n",
    "  * images -- segmentations: has an *identifier == a* for the case. For each case\n",
    "    * shapes and spacing are the same; channels are matched\n",
    "    * input channels must be consistent: same order and all channels are present\n",
    "  * images\n",
    "    * naming format: `{CASE_IDENTIFIER}_{XXXX}.{FILE_ENDING}.`\n",
    "    * arbitary input channels -> each input channel stored in *seperate image* \n",
    "    * Different input channels <-> same shape and spacing and co=registered(?)\n",
    "    * channels / modality are identified by their *FIILE_NAME_ENDING*: a four-digit integer(XXXX)\n",
    "    * There is a `dataset.json` connects channel names with identifiers in `channel_names` key\n",
    "    * Except for RGB: RGB's three channels can be stored in one file\n",
    "of the filename.\n",
    "  * segmentations\n",
    "    * naming format: `{CASE_IDENTIFER}.{FILE_ENDING}`\n",
    "    * integer maps with each value representing a semantic class\n",
    "    * share the same geometry(shape and spacing) with their corresponding images\n",
    "\n",
    "* Data format\n",
    "  * Same for all images and segmentations in train and test\n",
    "  * converting everything to .nii.gz!\n",
    "    * abstracting the input and output of images + segmentations through `BaseReaderWriter`\n",
    "    * [own data format support](../nnunetv2/imageio/readme.md)\n",
    "  * supports 2D input images: lossless compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset folder structure\n",
    "\n",
    "The dataset directory structure is like this:\n",
    "\n",
    "nnUNet_raw/\n",
    "    ├── Dataset001_BrainTumour(Dataset<XXX>_<dataset_name>)\n",
    "        ├── dataset.json\n",
    "        ├── imagesTr: trainging cases image(all operations on)\n",
    "            ├── la_003_0000.nii.gz\n",
    "            ├── la_004_0000.nii.gz\n",
    "            ├── ...\n",
    "        ├── imagesTs: test cases  # optional\n",
    "        └── labelsTr: images with ground truth for training cases\n",
    "            ├── la_003.nii.gz\n",
    "            ├── la_003.nii.gz\n",
    "            ├── ...\n",
    "    ├── Dataset002_Heart\n",
    "    ├── Dataset003_Liver\n",
    "    ├── Dataset004_Hippocampus\n",
    "    ├── Dataset005_Prostate\n",
    "    ├── ...\n",
    "\n",
    "The case is just like different definition of shape, including the number of channels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset.json\n",
    "\n",
    "* channel_name: id: name\n",
    "* labels: name <-> id \n",
    "* other essential params: like the type of the file...\n",
    "\n",
    "```\n",
    "channel_names:\n",
    "{\n",
    "    0: 'T1',\n",
    "    1: 'CT'\n",
    "}\n",
    "labels:\n",
    "(regular)\n",
    "{\n",
    "    'background': 0,\n",
    "    'left atrium': 1,\n",
    "    'some other label': 2\n",
    "}\n",
    "(region based)\n",
    "{\n",
    "    'background': 0,\n",
    "    'whole tumor': (1, 2, 3),\n",
    "    'tumor core': (2, 3),\n",
    "    'enhancing tumor': 3\n",
    "}\n",
    "```\n",
    "\n",
    "Use [here](../nnunetv2/dataset_conversion/generate_dataset_json.py) to generate dataset.json automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example function for generate dataset json\n",
    "from typing import Tuple\n",
    "from batchgenerators.utilities.file_and_folder_operations import save_json, join\n",
    "\n",
    "def generate_dataset_json(overwrite,\n",
    "                          output_folder: str,\n",
    "                          channel_names: dict,\n",
    "                          labels: dict,\n",
    "                          num_training_cases: int,\n",
    "                          file_ending: str,\n",
    "                          regions_class_order: Tuple[int, ...] = None,\n",
    "                          dataset_name: str = None, reference: str = None, release: str = None, license: str = None,\n",
    "                          description: str = None,\n",
    "                          overwrite_image_reader_writer: str = None, **kwargs):\n",
    "    json_exist = False\n",
    "    if os.path.exists(os.path.join(output_folder, \"dataset.json\")):\n",
    "        print(\"dataset.json already exists!\")\n",
    "        json_exist = True \n",
    "    if json_exist == False or overwrite == True:\n",
    "        has_regions: bool = any([isinstance(i, (tuple, list)) and len(i) > 1 for i in labels.values()])\n",
    "        if has_regions:\n",
    "            assert regions_class_order is not None, f\"You have defined regions but regions_class_order is not set. \" \\\n",
    "                                                    f\"You need that.\"\n",
    "        # channel names need strings as keys\n",
    "        # * reform the channel names dictionary to be \"string\": id\n",
    "        # * it is the value of \"channel names\" in json\n",
    "        keys = list(channel_names.keys())\n",
    "        for k in keys:\n",
    "            if not isinstance(k, str):\n",
    "                channel_names[str(k)] = channel_names[k]\n",
    "                del channel_names[k]\n",
    "\n",
    "        # * reform labels as ints (values)\n",
    "        for l in labels.keys():\n",
    "            value = labels[l]\n",
    "            if isinstance(value, (tuple, list)):\n",
    "                value = tuple([int(i) for i in value])\n",
    "                labels[l] = value\n",
    "            else:\n",
    "                labels[l] = int(labels[l])\n",
    "\n",
    "        dataset_json = {\n",
    "            'channel_names': channel_names,  # previously this was called 'modality'. I didn't like this so this is\n",
    "            # channel_names now. Live with it.\n",
    "            'labels': labels,\n",
    "            'numTraining': num_training_cases,\n",
    "            'file_ending': file_ending,\n",
    "        }\n",
    "\n",
    "        if dataset_name is not None:\n",
    "            dataset_json['name'] = dataset_name\n",
    "        if reference is not None:\n",
    "            dataset_json['reference'] = reference\n",
    "        if release is not None:\n",
    "            dataset_json['release'] = release\n",
    "        if license is not None:\n",
    "            dataset_json['licence'] = license\n",
    "        if description is not None:\n",
    "            dataset_json['description'] = description\n",
    "        if overwrite_image_reader_writer is not None:\n",
    "            dataset_json['overwrite_image_reader_writer'] = overwrite_image_reader_writer\n",
    "        if regions_class_order is not None:\n",
    "            dataset_json['regions_class_order'] = regions_class_order\n",
    "\n",
    "        dataset_json.update(kwargs)\n",
    "\n",
    "        save_json(dataset_json, join(output_folder, 'dataset.json'), sort_keys=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy image from origin dataset\n",
    "def copy_rename(old_dir, old_name, new_dir, new_name, delete_ori=False):\n",
    "    shutil.copy(os.path.join(old_dir, old_name), new_dir)\n",
    "    os.rename(os.path.join(new_dir, old_name), os.path.join(new_dir, new_name))\n",
    "    if delete_ori:\n",
    "        os.remove(os.path.join(old_dir, old_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the images' name is the same as the\n",
    "# requirement of nnUNet\n",
    "def check_format(image_name):\n",
    "    end = image_name.find(\".nii.gz\")\n",
    "    channel_name = image_name[end - 4 : end]\n",
    "    # print(channel_name)\n",
    "    # To see whether the four last characters is effect\n",
    "    for char in channel_name:\n",
    "        if not(ord(char) >= 48 and (ord(char)) <= 57):\n",
    "            # print(char, ord(char))\n",
    "            return False\n",
    "    return True \n",
    "def rename_for_channel(dir):\n",
    "    for file_name in os.listdir(dir):\n",
    "        if check_format(file_name) == False:\n",
    "            new_file_name = file_name[:file_name.find(\".nii.gz\")] + \"_0000.nii.gz\"\n",
    "            if os.path.exists(os.path.join(dir, new_file_name)):\n",
    "                os.remove(os.path.join(dir, file_name))\n",
    "            else:\n",
    "                os.rename(os.path.join(dir, file_name), os.path.join(dir, new_file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:\\dataset\\mindBoggle\\nnUNet\\nnUNet_raw\\Dataset001 exists.\n",
      "f:\\dataset\\mindBoggle\\nnUNet\\nnUNet_raw\\Dataset001\\imagesTr exists.\n",
      "f:\\dataset\\mindBoggle\\nnUNet\\nnUNet_raw\\Dataset001\\imagesTs exists.\n",
      "f:\\dataset\\mindBoggle\\nnUNet\\nnUNet_raw\\Dataset001\\labelsTr created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# * generate the right f\n",
    "nnunum_raw_data = os.path.join(root_dir, \"nnUNet_raw\")\n",
    "dataset_identifer = \"Dataset001\"\n",
    "dataset_path = os.path.join(nnunum_raw_data, dataset_identifer)\n",
    "train_case_path = os.path.join(dataset_path, \"imagesTr\")\n",
    "label_case_path = os.path.join(dataset_path, \"labelsTr\")\n",
    "test_case_path = os.path.join(dataset_path, \"imagesTs\")\n",
    "make_if_dont_exist(dataset_path)\n",
    "make_if_dont_exist(train_case_path)\n",
    "make_if_dont_exist(test_case_path)\n",
    "make_if_dont_exist(label_case_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# *if file exists in segementation and input at the same time, it is valid, add them in the train and lable dir\n",
    "ground_truth_files = os.listdir(ground_truth_dir)\n",
    "input_files = os.listdir(input_dir)\n",
    "ground_truth_files = [file_name for file_name in ground_truth_files if file_name.endswith(\".nii.gz\")]\n",
    "input_files = [file_name for file_name in input_files if file_name.endswith(\".nii.gz\")]\n",
    "\n",
    "for file_name in input_files:\n",
    "    if file_name in ground_truth_files:\n",
    "        copy_rename(ground_truth_dir, file_name, train_case_path, file_name)\n",
    "        copy_rename(input_dir, file_name, label_case_path, file_name)\n",
    "    else:\n",
    "        print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.json already exists!\n"
     ]
    }
   ],
   "source": [
    "# rename all the images\n",
    "rename_for_channel(train_case_path)\n",
    "# rename_for_channel(label_case_path)\n",
    "\n",
    "generate_dataset_json(\n",
    "    True,\n",
    "    output_folder = dataset_path,\n",
    "    channel_names = {0: \"MRI\"},\n",
    "    labels = {\"background\": 0,\n",
    "        \"Cortical gray matter\": 1,\n",
    "        \"Cortical White matter\": 2,\n",
    "        \"Cerebellum gray\" : 3,\n",
    "        \"Cerebellum white\" : 4},\n",
    "    num_training_cases = len(ground_truth_files),\n",
    "    file_ending = \".nii.gz\",\n",
    "    dataset_name = dataset_identifer,\n",
    "    overwrite_image_reader_writer='NibabelIOWithReorient' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:\\dataset\\mindBoggle\\nnUNet\\my_nnunet\\nnUNet_preprocessed exists.\n",
      "f:\\dataset\\mindBoggle\\nnUNet\\my_nnunet\\nnUNet_results exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# * preprocess and result file\n",
    "os.chdir(my_nnunet_dir)\n",
    "preprocess_path = os.path.join(my_nnunet_dir, \"nnUNet_preprocessed\")\n",
    "result_path = os.path.join(my_nnunet_dir, \"nnUNet_results\")\n",
    "make_if_dont_exist(preprocess_path)\n",
    "make_if_dont_exist(result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set environment variables\n",
    "\n",
    "See [here](documentation/set_environment_variables.md).\n",
    "\n",
    "env variables is for\n",
    "* raw, preprocessed data and trained models' position\n",
    "\n",
    "On Linux and Mac(permanent)\n",
    "```bash\n",
    "export nnUNet_raw=\"/media/fabian/nnUNet_raw\"\n",
    "export nnUNet_preprocessed=\"/media/fabian/nnUNet_preprocessed\"\n",
    "export nnUNet_results=\"/media/fabian/nnUNet_results\"\n",
    "```\n",
    "or to execute this to change it temporarily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['nnUNet_raw'] = nnunum_raw_data\n",
    "os.environ['nnUNet_preprocessed'] = preprocess_path\n",
    "os.environ['nnUNet_results'] = result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (467975115.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[38], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "nnUNetv2_plan_and_preprocess -d 1 --verify_dataset_integrity\n",
    "# nnunetv2.experiment_planning.plan_and_preprocess_entrypoints\n",
    "# position of the file \n",
    "# * nnUNetv2_plan_and_preprocess = \"nnunetv2.experiment_planning.plan_and_preprocess_entrypoints:plan_and_preprocess_entry\"\n",
    "# * dataset_fingerprint.json: shape and spacing\n",
    "# * nnUNetPlans.json: topoology\n",
    "# * preprocesses data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "# basic version\n",
    "nnUNetv2_train DATASET_NAME_OR_ID UNET_CONFIGURATION FOLD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train parameters\n",
    "\n",
    "`nnUNetv2_train = \"nnunetv2.run.run_training:run_training_entry\"`\n",
    "train's parameters settings\n",
    "nnunetv2.training.nnUNetTrainer\n",
    "* `UNET_CONFIGURATION`: requested U-Net configuration (defaults: 2d, 3d_fullres, 3d_lowres, 3d_cascade_lowres)\n",
    "* `--npz`: save the softmax outputs during the final validation\n",
    "* `--c`: continue a previous training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process of training\n",
    "\n",
    "[file place](nnunetv2/run/run_training.py) run.run_training.run_training_entry(): input argument => run_training\n",
    "run_training: assign input argument to the training setting(Using `get_trainer_from_args`): enable ddp => \n",
    "* class `nnunetv2.training.nnUNetTrainer.nnUNetTrainer`: initialize configuration, parameters; hyperparameters; plans for topology(will determine the structure of network after)\n",
    "*if enable training*\n",
    "nnUNetTrainer.run_training => on_trian_start: (preperation of train)\n",
    "1. initialize\n",
    "* network: utils.get_network_from_plans: dimension, plan, type of network to exiplicit network(the network structure is in dynamic_network_architectures: how to change the network is in [Extend](\"F:\\dataset\\mindBoggle\\nnUNet\\documentation\\extending_nnunet.md\"))\n",
    "* nnUNetTrainer.configure_optimizers: optimizer and scheduler(PolyLRScheduler(training.lr_sscheduler))\n",
    "* _build_loss: set the loss function as predifined `DC_and_CE_loss` + deepsupervision scale(?)(given from dataset): \n",
    "2. unpack dataset\n",
    "3. get dataloader: \n",
    "* how to rotate + transformation(append all tranformation in the self defined (trainsform) or transform in library)\n",
    "* use nnunet_dataset to get split of the dataset\n",
    "* use base_data_loader to load the bbox of the image: only seg the class out\n",
    "=> on_epoch_start: start to reord one epoch's info in logger\n",
    "=> train for one epoch normally => one_epoch_end: collect all the result in a list => valisation_epoch_start => validation_step => valistaion_epoch_end => on_epoch_end(log down info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### postprocessing and inference\n",
    "\n",
    "find best configuration [file place](nnunetv2/evaluation/find_best_configuration.py)\n",
    "\n",
    "`nnUNetv2_find_best_convfiguration = \"nnunetv2.evaluation.find_best_configuration:find_best_configuration_entry_point\"`\n",
    "It will return the best plan for the net(test among all the trained model) Parameters: \n",
    "* necessary param: \"dataset_name_or_id\n",
    "* chosen param: `-p`: plans `-c`: list of configurations(default is all of the mentioned configurations(need to change if there isn't enough config)), `-f`: fold to use(default 5 folds), '-tr': list of trainers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend network\n",
    "\n",
    "* Quick and dirty: implement a new nnUNetTrainer class and overwrite its `build_network_architecture` function. \n",
    "  Make sure your architecture is compatible with deep supervision (if not, use `nnUNetTrainerNoDeepSupervision`\n",
    "  as basis!) and that it can handle the patch sizes that are thrown at it! Your architecture should NOT apply any \n",
    "  nonlinearities at the end (softmax, sigmoid etc). nnU-Net does that!   \n",
    "* The 'proper' (but difficult) way: Build a dynamically configurable architecture such as the `PlainConvUNet` class(Maybe too complex to implement)\n",
    "  used by default. It needs to have some sort of GPU memory estimation method that can be used to evaluate whether \n",
    "  certain patch sizes and \n",
    "  topologies fit into a specified GPU memory target. Build a new `ExperimentPlanner` that can configure your new \n",
    "  class and communicate with its memory budget estimation. Run `nnUNetv2_plan_and_preprocess` while specifying your \n",
    "  custom `ExperimentPlanner` and a custom `plans_name`. Implement a nnUNetTrainer that can use the plans generated by \n",
    "  your `ExperimentPlanner` to instantiate the network architecture. Specify your plans and trainer when running `nnUNetv2_train`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse how the nnFormer is changing the nnUNet\n",
    "\n",
    "It is based on the V1 of nnUNet, so it needs some changes when reproduce. \n",
    "\n",
    "import network place: [here](F:\\dataset\\mindBoggle\\nnFormer\\nnformer\\training\\network_training\\nnFormerTrainerV2_CascadeFullRes.py) `nnTrainer` every new network structure will have its own nnTrainer, which is 继承 from nnTrainer. All files in the `network_training` dic is different Trainer for different network(or training strategy), including the nnUNetTrainerV2. What need to rewrite can refer to the example\n",
    "\n",
    "The import module is [class for network](F:\\dataset\\mindBoggle\\nnFormer\\nnformer\\network_architecture\\neural_network.py)`SegmentationNetwork`, the main structure of the net is rewritten and only some object of `SegmentationNetwork` is used.\n",
    "\n",
    "To compare the difference in nnTrainer, we start from checking the difference between [run](F:\\dataset\\mindBoggle\\nnFormer\\nnformer\\run\\run_training.py)\n",
    "\n",
    "The arguments for run_training(The same with the default version of nnUNetv1)\n",
    "necessary\n",
    "* network: i.e. configuration\n",
    "* network_trainer: i.e. using network trainer, including all info about the functions used in training\n",
    "* task: dataset_name_or_id\n",
    "* fold\n",
    "optional\n",
    "\n",
    "After assign for the variables, \n",
    "`get_default_configuration`\n",
    "* This function can get beck the needed file's name and the name of the network module. The network trianer will be built after return with other parameters grab by other func. Then using the netTrainer to train\n",
    "`trainer_class`\n",
    "`trainer.run_training()`\n",
    "`load_checkpoint`\n",
    "As long as the config for the trainer is right / the package loaded for the package is right\n",
    "\n",
    "Then goes to the `trainer.run_training`\n",
    "`do_ds`: whether do deep supervision\n",
    "in `initialize_network`, network is initialized and load weight. Network is suceeded from the `neural network.segmentation.xxxnnFormer`, which seems don't need any important features from it.\n",
    "For the `xxTrainer`, they are multi level success from the original network trainer. However, for this part, the new version is completely different. But for the old version, as all of the function is been packaged, most parts of train and validate is no need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your self net using nnUNet frame\n",
    "\n",
    "* nnTrainer\n",
    "  * Mostly don't need to change much. Just succeed the trainer model and change the function that you want to rewrite. Here are some functions that may be incompatible with the whole frame\n",
    "  * As long as suceed the origin class, all of the functions defined in that can use. We can also directly reload the funtion so that it can be replaced by ourselves.\n",
    "* Network\n",
    "  * for the initialization of the network, some of the parameters are predefined and is directly defined by the data and users. They are used in nnFormer, but definitely used in the final. Just first repeat what the assigned parameters are doing(in the nnFormerTrainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
